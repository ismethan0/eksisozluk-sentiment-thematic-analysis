"""
TestVeri_Duygulu.xlsx dosyasÄ±ndaki entry'leri okuyup 
duygu analizini yapÄ±p Twitter sÃ¼tununa yazan script
"""

import os
import sys
import time
import unicodedata
import pandas as pd
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeout

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# NLP servisini import et
from services.nlp_service import NLPService

def analyze_test_data(input_file='TestVeri_Duygulu.xlsx', output_file='TestVeri_Duygulu_Analyzed.xlsx', samples_per_category=None):
    """
    Excel dosyasÄ±ndaki entry'leri okuyup duygu ve tema analizi yap.
    Ä°steÄŸe baÄŸlÄ±: Her kategoriden dengeli sayÄ±da Ã¶rnek seÃ§er.
    
    SÃ¼tunlar:
        body: Analiz edilecek metin
        RDuygu: GerÃ§ek duygu etiketi
        Rkategori: GerÃ§ek kategori
        Tduygu: Model tahmini (0=negative, 1=neutral, 2=positive)
        Tkategori: Model kategori tahmini
        topic: GerÃ§ek kategori adÄ± (Rkategori'den kopyalanÄ±r)
    
    Args:
        input_file: Okunacak Excel dosyasÄ±
        output_file: SonuÃ§larÄ±n yazÄ±lacaÄŸÄ± Excel dosyasÄ±
        samples_per_category: Her kategoriden kaÃ§ Ã¶rnek alÄ±nacak.
                      None, 'all', 0 veya 'none' ise Ã¶rnekleme yapÄ±lmaz
                      ve tÃ¼m geÃ§erli satÄ±rlar iÅŸlenir.
    """
    print(f"ğŸ“– Reading file: {input_file}")
    
    try:
        # Excel dosyasÄ±nÄ± oku
        df = pd.read_excel(input_file)
        print(f"   Found {len(df)} rows")
        
        # SÃ¼tun isimlerini kontrol et
        print(f"   Columns: {list(df.columns)}")
        
        # Gerekli sÃ¼tunlarÄ± kontrol et
        required_cols = ['body', 'RDuygu', 'Rkategori']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"âŒ Gerekli sÃ¼tunlar bulunamadÄ±: {missing_cols}")
            print(f"   Mevcut sÃ¼tunlar: {list(df.columns)}")
            return
        
        # BoÅŸ deÄŸerleri filtrele
        df_clean = df[(df['body'].notna()) & (df['body'] != '') & 
                      (df['RDuygu'].notna()) & (df['RDuygu'] != '') &
                      (df['Rkategori'].notna()) & (df['Rkategori'] != '')].copy()
        
        print(f"   Valid rows after filtering: {len(df_clean)}")
        
        # Ã–rnekleme: isteÄŸe baÄŸlÄ±
        if samples_per_category is None or (isinstance(samples_per_category, int) and samples_per_category <= 0):
            print("\nğŸ“Š No sampling: processing all valid rows")
            df_sampled = df_clean.copy()
        else:
            print(f"\nğŸ“Š Sampling {samples_per_category} entries per category...")
            
            # Kategori daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
            category_counts = df_clean['Rkategori'].value_counts()
            print(f"   Available categories:")
            for cat, count in category_counts.items():
                print(f"      {cat}: {count} entries")
            
            # Her kategoriden Ã¶rnek seÃ§
            sampled_dfs = []
            for category in category_counts.index:
                cat_df = df_clean[df_clean['Rkategori'] == category]
                sample_size = min(int(samples_per_category), len(cat_df))
                sampled = cat_df.sample(n=sample_size, random_state=42)
                sampled_dfs.append(sampled)
                print(f"      Selected {sample_size} from {category}")
            
            # Ã–rnekleri birleÅŸtir ve karÄ±ÅŸtÄ±r
            df_sampled = pd.concat(sampled_dfs, ignore_index=True)
            df_sampled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"\n   Total samples selected: {len(df_sampled)}")
        
        # Yeni sÃ¼tunlar ekle
        df_sampled['Tduygu'] = ''
        df_sampled['Tkategori'] = ''
        df_sampled['topic'] = df_sampled['Rkategori']  # topic = Rkategori
        
        print(f"   Columns: body, RDuygu, Rkategori, topic")
        print(f"   Will predict: Tduygu, Tkategori")
        
        # NLP servisini baÅŸlat
        print("\nğŸ¤– Initializing NLP service...")
        nlp_service = NLPService()

        # Her Ã§aÄŸrÄ± iÃ§in zaman aÅŸÄ±mÄ± (saniye)
        try:
            per_call_timeout = int(os.getenv('NLP_TIMEOUT_SEC', '45'))
        except Exception:
            per_call_timeout = 45
        print(f"   Per-entry timeout: {per_call_timeout}s (set NLP_TIMEOUT_SEC to change)")

        def _analyze_one(text: str):
            return nlp_service.analyze_combined(text)
        
        # Her entry iÃ§in duygu ve tema analizi yap
        print(f"\nğŸ”¬ Analyzing {len(df_sampled)} entries...")
        
        sentiment_results = []
        category_results = []

        # Checkpoint ayarlarÄ±
        try:
            save_every = int(os.getenv('CHECKPOINT_EVERY', '20'))
        except Exception:
            save_every = 20
        checkpoint_path = os.getenv('CHECKPOINT_PATH', output_file.replace('.xlsx', '_partial.xlsx'))

        def _save_partial(k: int):
            try:
                tmp_df = df_sampled.copy()
                if k > 0:
                    idxs = tmp_df.index[:k]
                    tmp_df.loc[idxs, 'Tduygu'] = sentiment_results[:k]
                    tmp_df.loc[idxs, 'Tkategori'] = category_results[:k]
                tmp_df.to_excel(checkpoint_path, index=False)
                print(f"   ğŸ’¾ Checkpoint saved ({k} rows) -> {checkpoint_path}")
            except Exception as e:
                print(f"   âš ï¸ Checkpoint save failed: {e}")
        
        try:
            for idx, row in df_sampled.iterrows():
                body_text = str(row['body'])
                
                try:
                    # Hem duygu hem tema analizi yap (zaman aÅŸÄ±mÄ± ile)
                    start_ts = time.time()
                    # Her olasÄ± takÄ±lmada ana iÅŸ parÃ§acÄ±ÄŸÄ±nÄ± korumak iÃ§in tek kullanÄ±mlÄ±k executor
                    executor = ThreadPoolExecutor(max_workers=1)
                    future = executor.submit(_analyze_one, body_text)
                    try:
                        combined_result = future.result(timeout=per_call_timeout)
                    except FuturesTimeout:
                        print(f"   [Row {idx}] â³ Timeout after {per_call_timeout}s â€” skipping entry")
                        # Bu executor artÄ±k beklenmeden kapatÄ±lÄ±r; arka planda Ã§alÄ±ÅŸan thread bÄ±rakÄ±labilir
                        executor.shutdown(wait=False)
                        raise TimeoutError(f"analyze_combined timeout > {per_call_timeout}s")
                    except Exception as e_call:
                        executor.shutdown(wait=False)
                        raise e_call
                    else:
                        executor.shutdown(wait=True)
                    elapsed = time.time() - start_ts
                    if elapsed > per_call_timeout * 0.7:
                        print(f"   [Row {idx}] âš ï¸ Slow call took {elapsed:.1f}s")

                    # Duygu analizi sonucu -> 0, 1, 2 formatÄ±nda
                    sentiment = combined_result['sentiment']['sentiment']
                    sentiment_map = {
                        'negative': 0,
                        'neutral': 1,
                        'positive': 2
                    }
                    sentiment_code = sentiment_map.get(sentiment, 1)
                    
                    # Tema analizi sonucu
                    main_topic = combined_result['theme']['main_topic']
                    
                    sentiment_results.append(sentiment_code)
                    category_results.append(main_topic)
                    
                    # Ä°lerleme gÃ¶ster (her 5 kayÄ±tta bir)
                    if (len(sentiment_results)) % 5 == 0:
                        print(f"   Progress: {len(sentiment_results)}/{len(df_sampled)}", flush=True)

                    # Periyodik checkpoint
                    if save_every > 0 and (len(sentiment_results) % save_every == 0):
                        _save_partial(len(sentiment_results))
                
                except Exception as e:
                    print(f"   [Row {idx}] Error: {e}")
                    sentiment_results.append('')
                    category_results.append('')
                    if save_every > 0 and (len(sentiment_results) % save_every == 0):
                        _save_partial(len(sentiment_results))
        except KeyboardInterrupt:
            print("\nğŸ›‘ Interrupted by user. Saving checkpoint before exit...")
            _save_partial(len(sentiment_results))
            raise
        
        # SonuÃ§larÄ± sÃ¼tunlara yaz
        df_sampled['Tduygu'] = sentiment_results
        df_sampled['Tkategori'] = category_results
        
        print(f"\n   âœ… Analysis completed: {len(sentiment_results)} entries processed")
        
        # SonuÃ§larÄ± kaydet
        print(f"\nğŸ’¾ Saving results to: {output_file}")
        df_sampled.to_excel(output_file, index=False)
        
        # Ã–zet istatistikler
        metrics_lines = []
        def mprint(s: str):
            print(s)
            metrics_lines.append(s)
        total_samples = len(df_sampled)
        mprint("\nğŸ“Š Summary:")
        
        mprint("\n   Topic (Rkategori) Distribution:")
        topic_counts = df_sampled['topic'].value_counts()
        for topic, count in topic_counts.items():
            percentage = (count / total_samples) * 100
            mprint(f"      {topic}: {count} ({percentage:.1f}%)")
        
        mprint("\n   Tduygu Distribution:")
        sentiment_counts = df_sampled['Tduygu'].value_counts()
        sentiment_labels = {0: 'negative', 1: 'neutral', 2: 'positive'}
        for code, count in sentiment_counts.items():
            if code != '':
                percentage = (count / total_samples) * 100
                label = sentiment_labels.get(code, 'unknown')
                mprint(f"      {code} ({label}): {count} ({percentage:.1f}%)")
        
        mprint("\n   Tkategori Distribution:")
        category_counts = df_sampled['Tkategori'].value_counts()
        for category, count in category_counts.items():
            if category != '':
                percentage = (count / total_samples) * 100
                mprint(f"      {category}: {count} ({percentage:.1f}%)")
        
        # DoÄŸruluk hesaplama
        mprint(f"\nğŸ¯ Accuracy Metrics:")
        
        # RDuygu'yu 0,1,2 formatÄ±na Ã§evir (saÄŸlam normalize)
        # Not: 0=olumsuz, 1=nÃ¶tr, 2=olumlu
        rduygu_map = {
            'negative': 0, 'neg': 0, 'olumsuz': 0, '-1': 0, '0': 0,
            'neutral': 1, 'neu': 1, 'nÃ¶tr': 1, 'notr': 1, '1': 1,
            'positive': 2, 'pos': 2, 'olumlu': 2, '2': 2
        }
        
        # GeÃ§erli satÄ±rlarÄ± filtrele
        valid_mask = (df_sampled['Tduygu'] != '') & (df_sampled['RDuygu'] != '')
        valid_df = df_sampled[valid_mask].copy()
        
        if len(valid_df) > 0:
            # RDuygu'yu normalize et (int/float/string varyantlarÄ±nÄ± yakala)
            def _normalize_rduygu(x):
                try:
                    # SayÄ±sal ise doÄŸrudan kontrol et
                    if isinstance(x, (int, float)):
                        xi = int(x)
                        return xi if xi in (0, 1, 2) else -1
                    # Metinsel ise sÃ¶zlÃ¼kten eÅŸle
                    s = str(x).lower().strip()
                    return rduygu_map.get(s, -1)
                except Exception:
                    return -1

            valid_df['RDuygu_normalized'] = valid_df['RDuygu'].apply(_normalize_rduygu)
            
            # Sadece baÅŸarÄ±yla eÅŸleÅŸenleri al
            valid_df = valid_df[valid_df['RDuygu_normalized'].isin([0, 1, 2])]
            
            if len(valid_df) > 0:
                # Tduygu'yu gÃ¼venli ÅŸekilde int'e Ã§evir ve sadece 0/1/2 olanlarÄ± kullan
                valid_df['Tduygu_int'] = pd.to_numeric(valid_df['Tduygu'], errors='coerce')
                valid_df = valid_df[valid_df['Tduygu_int'].isin([0, 1, 2])]
                true_labels = valid_df['RDuygu_normalized'].astype(int)
                pred_labels = valid_df['Tduygu_int'].astype(int)
                
                mprint("\n   === SENTIMENT ACCURACY ===")
                
                # Accuracy hesapla
                correct = (true_labels == pred_labels).sum()
                total = len(valid_df)
                accuracy = correct / total
                
                mprint(f"   Total valid samples: {total}")
                mprint(f"   Correct predictions: {correct}")
                mprint(f"   Accuracy: {accuracy:.2%}")
                
                # Sklearn varsa detaylÄ± metrikler
                try:
                    from sklearn.metrics import classification_report, confusion_matrix

                    mprint("\nğŸ“ˆ Classification Report:")
                    target_names = ['negative (0)', 'neutral (1)', 'positive (2)']
                    try:
                        report_text = classification_report(true_labels, pred_labels, labels=[0, 1, 2], target_names=target_names, zero_division=0)
                        mprint(report_text)
                    except Exception as e:
                        mprint(f"   âš ï¸ Could not compute classification report: {e}")

                    mprint("\nğŸ”¢ Confusion Matrix:")
                    cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1, 2])

                    # Confusion matrix'i gÃ¼zel formatta yazdÄ±r
                    mprint(f"{'':15} {'Pred-0':>10} {'Pred-1':>10} {'Pred-2':>10}")
                    labels_text = ['True-0 (neg)', 'True-1 (neu)', 'True-2 (pos)']
                    for i, label in enumerate(labels_text):
                        row_str = f"{label:15}"
                        for j in range(3):
                            row_str += f"{cm[i][j]:>10}"
                        mprint(row_str)

                except ImportError:
                    mprint("\n   ğŸ’¡ Tip: Install scikit-learn for detailed metrics:")
                    mprint("      pip install scikit-learn")
                
                # Kategori doÄŸruluÄŸu (Rkategori vs Tkategori)
                mprint("\n   === CATEGORY ACCURACY ===")
                valid_cat_mask = (df_sampled['Tkategori'] != '') & (df_sampled['Rkategori'] != '')
                valid_cat_df = df_sampled[valid_cat_mask].copy()
                
                if len(valid_cat_df) > 0:
                    def _normalize(label: object) -> str:
                        text = str(label).strip().lower().replace('â€™', "'").replace('â€˜', "'")
                        text = unicodedata.normalize('NFKD', text)
                        return text.encode('ascii', 'ignore').decode()

                    valid_cat_df['Rkategori_norm'] = valid_cat_df['Rkategori'].apply(_normalize)
                    valid_cat_df['Tkategori_norm'] = valid_cat_df['Tkategori'].apply(_normalize)

                    true_cat = valid_cat_df['Rkategori_norm']
                    pred_cat = valid_cat_df['Tkategori_norm']
                    
                    cat_correct = (true_cat == pred_cat).sum()
                    cat_total = len(valid_cat_df)
                    cat_accuracy = cat_correct / cat_total
                    
                    mprint(f"   Total samples: {cat_total}")
                    mprint(f"   Correct predictions: {cat_correct}")
                    mprint(f"   Category Accuracy: {cat_accuracy:.2%}")
                    
                    try:
                        from sklearn.metrics import classification_report
                        mprint("\nğŸ“ˆ Category Classification Report:")
                        try:
                            cat_report_text = classification_report(true_cat, pred_cat, zero_division=0)
                            mprint(cat_report_text)
                        except Exception as e:
                            mprint(f"   âš ï¸ Could not compute category classification report: {e}")
                    except ImportError:
                        pass
                
            else:
                mprint("   âš ï¸ No valid RDuygu labels found after normalization")
        else:
            mprint("   âš ï¸ No valid samples found (both RDuygu and Tduygu must be non-empty)")
        
        # Metrikleri dosyaya kaydet
        base, ext = os.path.splitext(output_file)
        metrics_file = f"{base}_metrics.txt"
        try:
            with open(metrics_file, 'w', encoding='utf-8') as f:
                f.write("\n".join(metrics_lines) + "\n")
            print(f"\nğŸ“ Metrics saved to: {metrics_file}")
        except Exception as e:
            print(f"\nâš ï¸ Could not save metrics file: {e}")

        print(f"\nâœ… Analysis complete! Results saved to: {output_file}")
        
    except FileNotFoundError:
        print(f"âŒ File not found: {input_file}")
        print("   Please make sure the file exists in the current directory.")
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # Komut satÄ±rÄ±ndan dosya adÄ± ve Ã¶rnek sayÄ±sÄ± alÄ±nabilir
    input_file = sys.argv[1] if len(sys.argv) > 1 else 'TestVeri_Duygulu.xlsx'
    output_file = sys.argv[2] if len(sys.argv) > 2 else 'TestVeri_Duygulu_Analyzed.xlsx'
    if len(sys.argv) > 3:
        arg = sys.argv[3].strip().lower()
        if arg in ('all', 'none', '0'):
            samples = None
        else:
            try:
                samples = int(arg)
            except Exception:
                samples = None
    else:
        samples = None
    
    analyze_test_data(input_file, output_file, samples)
