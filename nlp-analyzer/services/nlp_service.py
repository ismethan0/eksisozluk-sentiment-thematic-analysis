"""
NLP Servis Katmanƒ±
Ger√ßek transformer modelleriyle duygu ve tema analizi
"""

import os
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification


class NLPService:
    """Sadece duygu ve tema analizi yapan NLP servis sƒ±nƒ±fƒ±"""
    
    def __init__(self):
        """Servis ba≈ülatƒ±cƒ± - Ger√ßek modelleri y√ºkle"""
        print("üîÑ Loading NLP models...")

        # Bu dosyanƒ±n konumuna g√∂re ../models dizinini belirle
        base_dir = os.path.dirname(os.path.abspath(__file__))
        self.model_cache_dir = os.path.join(base_dir, "..", "models")
        os.makedirs(self.model_cache_dir, exist_ok=True)
        print(f"  üìÇ Model cache directory: {self.model_cache_dir}")
        
        try:
            device = 0 if torch.cuda.is_available() else -1

            # Duygu analizi modeli - T√ºrk√ße XLM-RoBERTa
            print("  üì• Loading sentiment model: incidelen/xlm-roberta-base-turkish-sentiment-analysis")
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model="incidelen/xlm-roberta-base-turkish-sentiment-analysis",
                device=device,
                use_fast=False,              # Slow tokenizer kullan (bug workaround)
                cache_dir=self.model_cache_dir
            )
            print("  ‚úÖ Sentiment model loaded")
            
            # Tema/Konu analizi modeli - T√ºrk√ße haber sƒ±nƒ±flandƒ±rma (savasy)
            print("  üì• Loading topic model: savasy/bert-turkish-text-classification")
            self.topic_model_name = "savasy/bert-turkish-text-classification"
            self.topic_tokenizer = AutoTokenizer.from_pretrained(
                self.topic_model_name,
                cache_dir=self.model_cache_dir
            )
            self.topic_model = AutoModelForSequenceClassification.from_pretrained(
                self.topic_model_name,
                cache_dir=self.model_cache_dir
            )
            
            # T√ºm skorlarƒ± almak i√ßin return_all_scores=True
            self.topic_pipeline = pipeline(
                "text-classification",
                model=self.topic_model,
                tokenizer=self.topic_tokenizer,
                device=device,
                return_all_scores=True
            )

            # Modelin label -> insan okunur tema isimleri
            # Model ƒ∞ngilizce etiket d√∂nd√ºr√ºyor: world, economy, culture, health, politics, sport, technology
            self.topic_code_to_label = {
                "LABEL_0": "D√ºnya",
                "LABEL_1": "Ekonomi",
                "LABEL_2": "K√ºlt√ºr",
                "LABEL_3": "Saƒülƒ±k",
                "LABEL_4": "Siyaset",
                "LABEL_5": "Spor",
                "LABEL_6": "Teknoloji",
            }
            
            # ƒ∞ngilizce -> T√ºrk√ße mapping
            self.english_to_turkish = {
                "world": "D√ºnya",
                "economy": "Ekonomi",
                "culture": "K√ºlt√ºr",
                "health": "Saƒülƒ±k",
                "politics": "Siyaset",
                "sport": "Spor",
                "technology": "Teknoloji"
            }
            
            print("  ‚úÖ Topic model loaded")
            print("‚úÖ All NLP models loaded successfully!\n")
            
        except Exception as e:
            print(f"‚ùå Error loading models: {e}")
            raise
        
    def analyze_sentiment(self, text: str) -> dict:
        """
        Duygu analizi yap - XLM-RoBERTa modeli ile
        
        Args:
            text (str): Analiz edilecek metin
            
        Returns:
            dict: {
                'sentiment': 'positive'|'negative'|'neutral',
                'score': float (-1 ile 1 arasƒ±),
                'confidence': float (0 ile 1 arasƒ±),
                'label': str (modelin orijinal etiketi)
            }
        """
        try:
            # Basit karakter bazlƒ± kesme (token deƒüil ama pratik)
            if len(text) > 512:
                text = text[:512]
            
            # Model ile duygu analizi yap
            result = self.sentiment_pipeline(text)[0]
            
            # Model √ßƒ±ktƒ±sƒ±nƒ± normalize et
            label = result['label'].lower()
            confidence = float(result['score'])
            
            # Sentiment etiketini standartla≈ütƒ±r
            if 'pos' in label or 'olumlu' in label:
                sentiment = 'positive'
                score = confidence
            elif 'neg' in label or 'olumsuz' in label:
                sentiment = 'negative'
                score = -confidence
            else:
                sentiment = 'neutral'
                score = 0.0
            
            return {
                'sentiment': sentiment,
                'score': round(score, 2),
                'confidence': round(confidence, 2),
                'label': result['label']  # Orijinal model etiketi
            }
            
        except Exception as e:
            print(f"‚ùå Sentiment analysis error: {e}")
            return {
                'sentiment': 'neutral',
                'score': 0.0,
                'confidence': 0.0,
                'error': str(e)
            }
    
    def analyze_theme(self, text: str, threshold: float = 0.15) -> dict:
        """
        Tema analizi yap - T√ºrk√ße haber sƒ±nƒ±flandƒ±rma modeli (savasy/bert-turkish-text-classification) ile
        
        Args:
            text (str): Analiz edilecek metin
            threshold (float): Minimum tema skoru e≈üiƒüi (varsayƒ±lan: 0.15)
            
        Returns:
            dict: {
                'themes': [list of themes],      # ['Siyaset', 'Ekonomi', ...]
                'keywords': [list of keywords],  # ['ekonomi', 'piyasa', ...]
                'main_topic': str,               # 'Siyaset'
                'scores': { 'Siyaset': 0.92, ... },
                'is_ambiguous': bool             # Birden fazla y√ºksek skorlu tema var mƒ±?
            }
        """
        try:
            # Token bazlƒ± kesme (daha akƒ±llƒ±)
            tokens = self.topic_tokenizer.encode(text, add_special_tokens=True)
            if len(tokens) > 512:
                # Son 512 token'ƒ± al (genelde sonu√ß yazƒ±nƒ±n sonunda)
                tokens = tokens[-512:]
                text = self.topic_tokenizer.decode(tokens, skip_special_tokens=True)
            
            # D√∂nen yapƒ±: [[{'label': 'LABEL_0', 'score': ...}, ...]]
            raw_result = self.topic_pipeline(text)[0]
            
            # Skora g√∂re sƒ±rala (azalan)
            raw_result_sorted = sorted(raw_result, key=lambda x: x['score'], reverse=True)
            
            themes = []
            scores = {}
            
            # Threshold'u ge√ßen temalarƒ± al (max 3)
            for item in raw_result_sorted:
                code = item['label']
                score = float(item['score'])
                
                # E≈üik deƒüerini ge√ßenler
                if score >= threshold:
                    # ƒ∞ngilizce veya LABEL_X formatƒ±nƒ± T√ºrk√ße'ye √ßevir
                    human_label = self._get_turkish_label(code)
                    themes.append(human_label)
                    scores[human_label] = round(score, 2)
                    
                    if len(themes) >= 3:
                        break
            
            # Hi√ß tema bulunamadƒ±ysa en y√ºksek skorluyu al
            if not themes:
                best = raw_result_sorted[0]
                human_label = self._get_turkish_label(best['label'])
                themes = [human_label]
                scores = {human_label: round(float(best['score']), 2)}
            
            main_topic = themes[0] if themes else 'Genel'
            
            # Belirsizlik kontrol√º (birden fazla yakƒ±n skorlu tema varsa)
            is_ambiguous = False
            if len(themes) >= 2:
                top_score = scores[themes[0]]
                second_score = scores[themes[1]]
                # Fark 0.1'den k√º√ß√ºkse belirsiz
                is_ambiguous = (top_score - second_score) < 0.1
            
            # Geli≈ümi≈ü keyword extraction
            keywords = self._extract_keywords(text, n=8)
            
            return {
                'themes': themes,
                'keywords': keywords,
                'main_topic': main_topic,
                'scores': scores,
                'is_ambiguous': is_ambiguous,
                'threshold_used': threshold
            }
            
        except Exception as e:
            print(f"‚ùå Theme analysis error: {e}")
            return {
                'themes': ['Genel'],
                'keywords': [],
                'main_topic': 'Genel',
                'scores': {},
                'is_ambiguous': False,
                'error': str(e)
            }
    
    def _get_turkish_label(self, label: str) -> str:
        """
        Model etiketini T√ºrk√ße'ye √ßevir
        
        Args:
            label (str): Model etiketi (LABEL_0, world, economy, vb.)
            
        Returns:
            str: T√ºrk√ße etiket
        """
        # √ñnce LABEL_X formatƒ±nƒ± kontrol et
        if label in self.topic_code_to_label:
            return self.topic_code_to_label[label]
        
        # ƒ∞ngilizce etiketi kontrol et (k√º√ß√ºk harfe √ßevir)
        label_lower = label.lower().strip()
        if label_lower in self.english_to_turkish:
            return self.english_to_turkish[label_lower]
        
        # Bulunamazsa capitalize et ve d√∂nd√ºr
        return label.capitalize()
    
    def _extract_keywords(self, text: str, n: int = 8) -> list:
        """
        Geli≈ümi≈ü keyword extraction - T√ºrk√ße i√ßin optimize edilmi≈ü
        
        Args:
            text (str): Metin
            n (int): Ka√ß keyword √ßƒ±karƒ±lacak
            
        Returns:
            list: Anahtar kelimeler
        """
        # Geni≈ületilmi≈ü T√ºrk√ße stop-words
        stop_words = {
            'bir', 've', 'bu', 'da', 'de', 'i√ßin', 'ile', 'mi', 'mƒ±', 'mu', 'm√º',
            'daha', '√ßok', 'ama', 'ya', 'gibi', '≈üu', 'o', 'ki', 'her', 'ne', 
            'var', 'yok', 'ben', 'sen', 'biz', 'siz', 'onlar', '≈üey', 'kadar',
            'sonra', '√∂nce', 'artƒ±k', 'hen√ºz', 'bile', 'sadece', 'ancak', 'veya',
            'ise', 'eƒüer', 'nasƒ±l', 'neden', 'ni√ßin', 'nerede', 'ne zaman',
            'hi√ß', 'bazen', 'belki', 'mutlaka', 'kesinlikle', 'zaten', 'aslƒ±nda',
            'yani', 'mesela', '√∂rneƒüin', '≈üimdi', 'b√∂yle', '≈ü√∂yle', 'benim',
            'senin', 'onun', 'bizim', 'sizin', 'diye', 'demek', 'olmak',
            'etmek', 'yapmak', 'vermek', 'almak', 'g√∂rmek', 'buna', '≈üunu',
            'bunu', 'bunun', '≈üunun', 'onun', 'olan', 'oldu', 'olur', 'olarak'
        }
        
        # Kelime ayƒ±rma ve temizleme
        import re
        words = re.findall(r'\b[a-z√ßƒüƒ±√∂≈ü√º]+\b', text.lower())
        
        # Filtreleme
        keywords = [w for w in words 
                   if len(w) > 3 and w not in stop_words and not w.isdigit()]
        
        # Frekans analizi
        from collections import Counter
        keyword_counts = Counter(keywords)
        
        # En sƒ±k ge√ßen n kelime
        top_keywords = [word for word, count in keyword_counts.most_common(n)]
        
        return top_keywords
    
    def analyze_combined(self, text: str) -> dict:
        """
        Hem duygu hem tema analizini birlikte yap
        
        Args:
            text (str): Analiz edilecek metin
            
        Returns:
            dict: {
                'sentiment': {...},
                'theme': {...}
            }
        """
        sentiment = self.analyze_sentiment(text)
        theme = self.analyze_theme(text)
        
        return {
            'sentiment': sentiment,
            'theme': theme
        }
